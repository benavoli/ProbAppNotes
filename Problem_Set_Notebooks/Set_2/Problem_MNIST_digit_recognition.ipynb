{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem MNIST digit recognition\n",
    "\n",
    "The goal of this notebook is to compare logistic regression and Bayesian logistic regression in the problem of digits recognition.\n",
    "\n",
    "In particular, our goal is to implement a binary classification that is able to distinguish \n",
    "the digits 5 versus 8. I have implemented the logistic regression in Sklearn for you. Your goal is to implement Bayesian Logistic regression (similarly to the one used for the Titanic dataset) using PyMC3. You have to compute the accuracy of the Bayesian logistic classifier (use the posterior mean for prediction) in the Test set and compare it with that of Sklearn.\n",
    "\n",
    "More important, you have to use the uncertainty to highlight the instances in the test set that are more difficult to classify for the Bayesian classifier. Each sampled probability determines a prediction, class 0 and class 1. A way\n",
    "to evaluate the uncertainty on the prediction for a certain instance in the test set is to compute\n",
    "the standard deviation of these different predictions.\n",
    "If the standard deviation is small, then it means that the decision was always class 0 (or class 1), while if the standard deviation  is high it means that the probabilistic classifier is undecided about how to classify that instance. \n",
    "\n",
    "1. plot this uncertainty against the mean predicted probability as we did for the Titanic dataset\n",
    "2. plot this uncertainty against the  predicted probability returned by Sklearn as we did for the Titanic dataset\n",
    "\n",
    "What do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "import random\n",
    "import arviz as az\n",
    "import theano as tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. \n",
    "\n",
    "You can download it from here\n",
    "\n",
    "https://osf.io/jda6s/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = loadmat(\"../datasets/mnist-original.mat\")\n",
    "mnist_data = mnist[\"data\"].T\n",
    "mnist_label = mnist[\"label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 9., 9., 9.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We only consider two digits to perform binary classification and only use 100 instances\n",
    "#per digit to make learning faset\n",
    "np.random.seed(0)\n",
    "digits = [5,8]\n",
    "N_per_digit =500\n",
    "X = []\n",
    "labels = []\n",
    "for d in digits:\n",
    "    imgs = mnist_data[np.where(mnist_label==d)[0],:]\n",
    "    X.append(imgs[np.random.permutation(imgs.shape[0]),:][0:N_per_digit,:])\n",
    "    labels.append(np.ones(N_per_digit)*d)\n",
    "X = np.vstack(X).astype(np.float64)\n",
    "y = np.hstack(labels)\n",
    "X = X / 255.0\n",
    "y = np.where(y==5,1,0)#class 3 ->1, class 8->0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data in training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 784)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn  Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "Accuracy= 0.9\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, C=900, max_iter=2000, solver='lbfgs').fit(X_train, y_train)\n",
    "y_pred_LR = clf.predict(X_test)\n",
    "y_pred_prob_LR = clf.predict_proba(X_test)[:,1]#class 1 probability\n",
    "print(y_pred_LR[0:3])\n",
    "print(\"Accuracy=\",accuracy_score(y_pred_LR,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
