{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Spam Classification\n",
    "Deciding whether an email is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#load dataset\n",
    "df=pd.read_csv('datasets/spam.csv', encoding='latin-1')\n",
    "df=df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis='columns')\n",
    "\n",
    "#df[v1] is the class variable and df[v2] is the  email\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 pre-processing: removing stopwords and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer('english')\n",
    "#A  stemming algorithm reduces words like fishing, fished, and fisher to the stem fish.\n",
    "#The stem need not be a word, for example  argue, argued, \n",
    "#argues, arguing, and argus could be reduced to the stem argu. \n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "#Stop words are  the most common words in a language\n",
    "#and are filtered out before processing of natural language data \n",
    "\n",
    "\n",
    "df['v2']=[re.sub('[^a-zA-Z]', ' ', sms) for sms in df['v2']]\n",
    "word_list=[sms.split() for sms in df['v2']]\n",
    "def normalize(words):\n",
    "    current_words=list()\n",
    "    for word in words:\n",
    "        if word.lower() not in stop: #remove  the most common words\n",
    "            updated_word=stemmer.stem(word) #stemming\n",
    "            current_words.append(updated_word.lower()) #lower case\n",
    "    return current_words\n",
    "word_list=[normalize(word) for word in word_list]\n",
    "df['v2']=[\" \".join(word) for word in word_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri wkli comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>nd time tri contact u u pound prize claim easi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>b go esplanad fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>piti mood suggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>guy bitch act like interest buy someth els nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  go jurong point crazi avail bugi n great world...\n",
       "1      ham                              ok lar joke wif u oni\n",
       "2     spam  free entri wkli comp win fa cup final tkts st ...\n",
       "3      ham                u dun say earli hor u c alreadi say\n",
       "4      ham               nah think goe usf live around though\n",
       "...    ...                                                ...\n",
       "5567  spam  nd time tri contact u u pound prize claim easi...\n",
       "5568   ham                              b go esplanad fr home\n",
       "5569   ham                                  piti mood suggest\n",
       "5570   ham  guy bitch act like interest buy someth els nex...\n",
       "5571   ham                                     rofl true name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[v1] is the class variable and df[v2] is the processed email\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in training and testing\n",
    "x_train, x_test, y_train, y_test=train_test_split(df['v2'], df['v1'], test_size=0.2, random_state=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ham'],\n",
       "       ['ham'],\n",
       "       ['ham'],\n",
       "       ...,\n",
       "       ['ham'],\n",
       "       ['spam'],\n",
       "       ['ham']], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: transforming email into counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails= 4457\n",
      "number of words= 5595\n",
      "  (0, 5405)\t1\n",
      "  (0, 1991)\t2\n",
      "  (0, 1140)\t1\n",
      "  (0, 3047)\t1\n",
      "  (0, 4944)\t1\n",
      "  (0, 3328)\t2\n",
      "  (0, 162)\t1\n",
      "  (0, 4483)\t1\n",
      "  (0, 1398)\t1\n",
      "  (0, 1921)\t1\n",
      "  (0, 2676)\t1\n",
      "  (0, 458)\t1\n",
      "  (0, 4620)\t1\n",
      "  (0, 1552)\t1\n",
      "  (0, 3214)\t1\n",
      "  (0, 1790)\t1\n",
      "  (0, 2541)\t1\n",
      "  (0, 4984)\t1\n",
      "  (0, 4456)\t1\n",
      "  (1, 1089)\t1\n",
      "  (1, 1734)\t1\n",
      "  (1, 150)\t1\n",
      "  (1, 156)\t1\n",
      "  (2, 3342)\t1\n",
      "  (2, 4701)\t1\n",
      "  :\t:\n",
      "  (4453, 74)\t1\n",
      "  (4454, 2580)\t1\n",
      "  (4454, 1623)\t1\n",
      "  (4454, 2920)\t1\n",
      "  (4454, 1681)\t1\n",
      "  (4454, 3715)\t1\n",
      "  (4454, 683)\t1\n",
      "  (4455, 1529)\t1\n",
      "  (4455, 5150)\t2\n",
      "  (4455, 1776)\t1\n",
      "  (4455, 5065)\t1\n",
      "  (4455, 1885)\t1\n",
      "  (4455, 4786)\t1\n",
      "  (4455, 1792)\t1\n",
      "  (4455, 3249)\t1\n",
      "  (4455, 3960)\t1\n",
      "  (4455, 2160)\t1\n",
      "  (4455, 2341)\t1\n",
      "  (4455, 5327)\t1\n",
      "  (4455, 4533)\t1\n",
      "  (4455, 4956)\t3\n",
      "  (4455, 3032)\t1\n",
      "  (4455, 5067)\t1\n",
      "  (4455, 3248)\t1\n",
      "  (4456, 1885)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it counts the words\n",
    "cv=CountVectorizer()\n",
    "#it returns the number of times a word appears in the i-th email\n",
    "x_train_df=cv.fit_transform(x_train) #x_train_df is a matrix emails times words\n",
    "print(\"number of emails=\",x_train_df.shape[0])\n",
    "print(\"number of words=\",x_train_df.shape[1])\n",
    "x_test_df=cv.transform(x_test)\n",
    "#this is a sparse matrix (it means that only non-zeroes elements are stored)\n",
    "print(x_train_df)\n",
    "#to get the full matrix\n",
    "x_train_df.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listv = cv.get_feature_names()\n",
    "#listv.append('Class')\n",
    "#dataset = pd.DataFrame(data=np.hstack([x_train_df.toarray(),y_train.values.reshape(-1,1)]), columns=listv)\n",
    "#dataset.to_csv(\"email_clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the input data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5595)\n",
      "this is the non-sparse matrix= [[0 0 0 ... 0 0 0]]\n",
      "\n",
      "wish great day moji told offer alway speechless offer easili go great length behalf stun exam next friday keep touch sorri\n",
      "\n",
      "[array(['alway', 'behalf', 'day', 'easili', 'exam', 'friday', 'go',\n",
      "       'great', 'keep', 'length', 'moji', 'next', 'offer', 'sorri',\n",
      "       'speechless', 'stun', 'told', 'touch', 'wish'], dtype='<U34')]\n",
      "\n",
      "[ 162  458 1140 1398 1552 1790 1921 1991 2541 2676 3047 3214 3328 4456\n",
      " 4483 4620 4944 4984 5405]\n",
      "\n",
      "[[1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "row_index=0 #select one email\n",
    "print(x_train_df[row_index,:].todense().shape)\n",
    "print(\"this is the non-sparse matrix=\",x_train_df[row_index,:].todense())\n",
    "ind=np.where(x_train_df[row_index,:].todense()[0,:]>0)[1]\n",
    "print()\n",
    "#original words in the email\n",
    "print(x_train.values[row_index])\n",
    "print()\n",
    "#decoded numerical input \n",
    "print(cv.inverse_transform(x_train_df[row_index,:].todense()))\n",
    "print()\n",
    "#index of those words in x_train_df[row_index,:].todense()\n",
    "print(ind)\n",
    "print()\n",
    "# number of times those words appears in the email\n",
    "print(x_train_df[row_index,ind].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: training the classifier and making predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'ham' ... 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train_df,y_train)\n",
    "prediction_train=clf.predict(x_train_df)\n",
    "prediction_test=clf.predict(x_test_df)\n",
    "print(prediction_test)#returns the predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99224585, 0.00775415],\n",
       "       [0.99710212, 0.00289788],\n",
       "       [0.99977543, 0.00022457],\n",
       "       ...,\n",
       "       [0.99721402, 0.00278598],\n",
       "       [0.99999219, 0.00000781],\n",
       "       [0.99364559, 0.00635441]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "proba=clf.predict_proba(x_test_df)#returns the rpedicted probability of Ham and Spam for each email in the test set\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['break time one come n get stuff fr'\n",
      " 'dun need use dial juz open da browser n surf' 'make babi yo tho'\n",
      " 'shower babi'\n",
      " 'regist sinco paye log icicibank com enter urn lt gt confirm bewar fraud share disclos urn anyon'\n",
      " 'yes sura sun tv lol'\n",
      " 'hey guy know breath neck get bud anyway abl get half track usf tonight'\n",
      " 'leav wif lar wan carri meh heavi da num familiar'\n",
      " 'feb lt gt love u day send dis ur valu frnds evn come back u gt marri person u luv u ignor dis u lose ur luv evr'\n",
      " 'gram usual run like lt gt half eighth smarter though get almost whole second gram lt gt'\n",
      " 'watch tv got new job' 'prabha soryda reali frm heart sori'\n",
      " 'forward hi mailbox messag sms alert messag match pleas call back retriev messag match'\n",
      " 'normal hot mail com see' 'oh yes like tortur watch england'\n",
      " 'hey rite u put evey mnth' 'good good job like entrepreneur'\n",
      " 'gr see messag r u leav congrat dear school wat r ur plan' 'late'\n",
      " 'let know detail fri u find cos tom fri mention chines thank'\n",
      " 'ah see lingo let know wot earth finish make'\n",
      " 'hey hun onbus goin meet want go meal donyt feel like cuz get last bus home hes sweet latelyxxx'\n",
      " 'ok let u noe leav hous' 'movi laptop' 'anyth lor' 'leav hous'\n",
      " 'ask around lot term mid'\n",
      " 'hope alright babe worri might felt bit despar learn job fake wait come back love'\n",
      " 'aiyar sorri lor forgot tell u'\n",
      " 'night end anoth day morn come special way may smile like sunni ray leav worri blue blue bay gud mrng'\n",
      " 'yet chikku k wat abt tht guy stop irrit msging u' 'good even ttyl'\n",
      " 'k much th fifti' 'also cbe pay'\n",
      " 'babe hope ok shit night sleep fell asleep knacker dread work tonight thou upto tonight x'\n",
      " 'sorri gone place tomorrow realli sorri'\n",
      " 'lick everi drop readi use mouth well' 'also andro ice etc etc'\n",
      " 'want cock hubbi away need real man satisfi txt wife string action txt stop end txt rec ea otbox la ws'\n",
      " 'aight grab someth eat text back mu'\n",
      " 'search happi main sourc unhappi accept life way come u find happi everi moment u live'\n",
      " 'carri disturb' 'month password wap mobsi com use wap phone pc' 'got meh'\n",
      " 'said text one time' 'sound good keep post'\n",
      " 'wah lucki man save money hee'\n",
      " 'turn phone mom tell everyon cancer sister stop call hurt talk put see u u get home love u'\n",
      " 'save money wed lingeri www bridal petticoatdream co uk choos superb select nation deliveri brought weddingfriend'\n",
      " 'k k congratul' 'dont know oh hope month'\n",
      " 'sms ac jsco energi high u may know channel day ur leadership skill r strong psychic repli an w question end repli end jsco'\n",
      " 'much torch ja' 'haha get use drive usf man know lot stoner'\n",
      " 'sorri batteri die come get gram place'\n",
      " 'still area restaur ill tri come back soon'\n",
      " 'know god creat gap finger one made come amp fill gap hold hand love'\n",
      " 'like dis sweater fr mango size alreadi irrit' 'husband'\n",
      " 'decemb mobil mths entitl updat latest colour camera mobil free call mobil updat vco free'\n",
      " 'dont shall buy one dear'\n",
      " 'parent kidz friend n colleagu scream surpris wait sofa nake'\n",
      " 'hiya probabl come home weekend next'\n",
      " 'shop till u drop either k k cash travel voucher call ntt po box cr bt fixedlin cost ppm mobil vari'\n",
      " 'way tirupur' 'went pay rent go bank authoris payment'\n",
      " 'hey sathya till dint meet even singl time saw situat sathya'\n",
      " 'stitch trouser' 'talk that de wont'\n",
      " 'hungri gay guy feel hungri call p min stop text call p min'\n",
      " 'hello peach cake tast lush' 'unlimit text limit minut'\n",
      " 'check got detail messag' 'guy get use dumb realiz'\n",
      " 'go buy mum present ar' 'ding ya break fassyol blacko londn'\n",
      " 'free rington text first poli text get true tone help st free tone x pw e nd txt stop'\n",
      " 'wait min stand bus stop'\n",
      " 'good even sir al salam wahleykkum share happi news grace god got offer tayseer tissco join hope fine inshah allah meet sometim rakhesh visitor india'\n",
      " 'dont worri day big lambu ji vl come til enjoy batchlor parti'\n",
      " 'xmas stori peac xmas msg love xmas miracl jesus hav bless month ahead amp wish u merri xmas'\n",
      " 'wuld without babi thought alon mite break wanna go crazi everyboy need ladi xxxxxxxx'\n",
      " 'total video convert free download type googl search'\n",
      " 'sorri dude dont know forgot even dan remind sorri hope guy fun'\n",
      " 'u still paint ur wall'\n",
      " 'pleas call custom servic repres freephon pm guarante cash prize'\n",
      " 'stop stori told return say order'\n",
      " 'dip cell dead come u better respond els shall come back'\n",
      " 'join lt gt bus' 'good r u work' 'lol alway convinc' 'bring home wendi'\n",
      " 'ur cash balanc current pound maxim ur cash send cash p msg cc hg suit land row w j hl'\n",
      " 'weird know one point' 'serious spell name'\n",
      " 'say slowli god love amp need clean heart blood send ten special peopl amp u c miracl tomorrow pls pls'\n",
      " 'yes watch footi worri go blow phil nevill'\n",
      " 'u wake alreadi thanx e tau sar piah quit nice'\n",
      " 'cant pick phone right pls send messag'\n",
      " 'yeah usual guy town definit peopl around know'\n",
      " 'good let thank god pleas complet drug lot water beauti day' 'ok bag'\n",
      " 'urgent week free membership prize jackpot txt word claim c www dbuk net lccltd pobox ldnw rw'\n",
      " 'urgent call landlin complimentari ibiza holiday cash await collect sae cs po box sk wp ppm'\n",
      " 'get offici england poli rington colour flag yer mobil tonight game text tone flag optout txt eng stop box w wx'\n",
      " 'borrow ur bag ok' 'chennai velacheri' 'mean websit yes'\n",
      " 'winner valu network custom select receivea prize reward claim call claim code kl valid hour'\n",
      " 'gonna worri noth give money use'\n",
      " 'way home went chang batt watch go shop bit lor' 'happen yo date'\n",
      " 'sms servic inclus text credit pls goto www comuk net login unsubscrib stop extra charg help po box ip'\n",
      " 'ola would get back mayb today told direct link us get car bid onlin arrang ship get cut u partnership u invest money ship take care rest u wud b self reliant soon dnt worri'\n",
      " 'like call mittelschmertz googl dont paracetamol dont worri go'\n",
      " 'life face choic toss coin becoz settl question coin air u know heart hope gudni'\n",
      " 'home way'\n",
      " 'warner villag c colin farrel swat wkend warner villag get free med popcorn show msg ticket kiosk valid c c kiosk repli soni mre film offer'\n",
      " 'got anoth job one hospit data analysi someth start monday sure thesi got finish'\n",
      " 'told go got drunk'\n",
      " 'feb lt gt love u day send dis ur valu frnds evn come back u gt marri person u luv u ignor dis u lose ur luv evr'\n",
      " 'k u bore come home'\n",
      " 'urgent mobil number award bonus caller prize call land line valid hrs'\n",
      " 'finish lunch alreadi u wake alreadi'\n",
      " 'met alex nichol middl school turn deal'\n",
      " 'good afternoon babe goe day job prospect yet miss love sigh'\n",
      " 'luton ring ur around h'\n",
      " 'cours math one day one chapter one month finish'\n",
      " 'congratul u claim vip row ticket c blu concert novemb blu gift guarante call claim ts cs www smsco net cost max'\n",
      " 'look addi goe back monday suck' 'finish eat got u plate leftov time'\n",
      " 'alright hook guy' 'u got person stori'\n",
      " 'supervisor find one lor thk student havent ask yet tell u aft ask'\n",
      " 'lol oop sorri fun' 'man bus slow think gonna get'\n",
      " 'reach home tire come tomorro' 'ok chikku favourit song'\n",
      " 'yo game almost want go walmart soon'\n",
      " 'though shd go n fun bar town someth sound ok'\n",
      " 'yalru lyfu astn chikku bt innu mundh lyf ali halla ke bilo marriag program edha lyf nt yet ovr chikku ali vargu lyfu meow meow'\n",
      " 'loan purpos homeown tenant welcom previous refus still help call free text back help'\n",
      " 'realli crash cuddl sofa']\n"
     ]
    }
   ],
   "source": [
    "ind=np.where(proba[:,1]>0.5)[0]#emails that are classified as Spam\n",
    "print(x_train.values[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: computing accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9923715503702042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#accuracy training set\n",
    "print(\"Accuracy:\"+str(accuracy_score(y_train,prediction_train)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We care about the generalisation error, that is the performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.989237668161435\n",
      "\n",
      "Confusion Matrix\n",
      "[[965   5]\n",
      " [  7 138]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#accuracy test set\n",
    "print(\"Accuracy:\"+str(accuracy_score(y_test,prediction_test)))\n",
    "print()\n",
    "\n",
    "conf_mat=confusion_matrix(y_test, prediction_test)\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Where can we find sparse matrices ?\n",
    "You can manipulate them using scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexes of non-zeroes elements= [ 162  458 1140 1398 1552 1790 1921 1991 2541 2676 3047 3214 3328 4456\n",
      " 4483 4620 4944 4984 5405]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sc #this is the library\n",
    "\n",
    "#x_train_df is a scipy sparse matrix, this avoids to store the zeroes\n",
    "#to access to the non-zero element\n",
    "i=0# email index\n",
    "ind=sc.find(x_train_df[i,:]>0)[1]\n",
    "print(\"indexes of non-zeroes elements=\",ind)\n",
    "x_train_df[0,ind].todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexes of non-zeroes elements= [2870 3588]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test set\n",
    "ind=sc.find(x_test_df[i,:]>0)[1]\n",
    "print(\"indexes of non-zeroes elements=\",ind)\n",
    "x_test_df[0,ind].todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider Movie Reviews Corpus, a dataset that includes  movie reviews that are categorized as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/benavoli/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "df = pd.DataFrame(columns=['v1', 'v2'])\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        df=df.append({'v1': category, 'v2': movie_reviews.words(fileid)}, ignore_index=True)\n",
    "        \n",
    "word_list=[sms for sms in df['v2']]\n",
    "def normalize(words):\n",
    "    current_words=list()\n",
    "    for word in words:\n",
    "        if word.lower() not in stop: #remove  the most common words\n",
    "            if word.isalpha(): #remove punctuation\n",
    "                updated_word=stemmer.stem(word) #stemming\n",
    "                current_words.append(updated_word.lower()) #lower case\n",
    "    return current_words\n",
    "word_list=[normalize(word) for word in word_list]\n",
    "df['v2']=[\" \".join(word) for word in word_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>plot two teen coupl go church parti drink driv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>happi bastard quick movi review damn bug got h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>movi like make jade movi viewer thank invent t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>quest camelot warner bros first featur length ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>synopsi mental unstabl man undergo psychothera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>pos</td>\n",
       "      <td>wow movi everyth movi funni dramat interest we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>pos</td>\n",
       "      <td>richard gere command actor alway great film ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>pos</td>\n",
       "      <td>glori star matthew broderick denzel washington...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>pos</td>\n",
       "      <td>steven spielberg second epic film world war ii...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>pos</td>\n",
       "      <td>truman true man burbank perfect name jim carre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v1                                                 v2\n",
       "0     neg  plot two teen coupl go church parti drink driv...\n",
       "1     neg  happi bastard quick movi review damn bug got h...\n",
       "2     neg  movi like make jade movi viewer thank invent t...\n",
       "3     neg  quest camelot warner bros first featur length ...\n",
       "4     neg  synopsi mental unstabl man undergo psychothera...\n",
       "...   ...                                                ...\n",
       "1995  pos  wow movi everyth movi funni dramat interest we...\n",
       "1996  pos  richard gere command actor alway great film ev...\n",
       "1997  pos  glori star matthew broderick denzel washington...\n",
       "1998  pos  steven spielberg second epic film world war ii...\n",
       "1999  pos  truman true man burbank perfect name jim carre...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same steps as in the Spam filter example, apply MultinomialNB to this example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
