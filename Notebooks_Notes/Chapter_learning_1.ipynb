{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0\n",
      "> 4\n"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "#Generative model\n",
    "#we select probabilities for the six faces\n",
    "theta = [0.1,0.2,0.2,0.2,0.2,0.1]\n",
    "#we define a probabilistic model\n",
    "dice = pm.Categorical.dist(theta)\n",
    "print(\">\",dice.random())\n",
    "print(\">\",dice.random())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [0.01 0.02 0.02 0.02 0.02 0.01 0.02 0.04 0.04 0.04 0.04 0.02 0.02 0.04\n",
      " 0.04 0.04 0.04 0.02 0.02 0.04 0.04 0.04 0.04 0.02 0.02 0.04 0.04 0.04\n",
      " 0.04 0.02 0.01 0.02 0.02 0.02 0.02 0.01]\n",
      "> (3, 3)\n",
      "> (2, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Possibility space of two rolls\n",
    "Omega=[(i,j) for i in range(1,7) for j in range(1,7)]\n",
    "#we select probabilities for the six faces\n",
    "theta = np.array([0.1,0.2,0.2,0.2,0.2,0.1])\n",
    "theta_2=np.kron(theta,theta)#this computes all possible products\n",
    "print(\">\",theta_2)\n",
    "#we define a probabilistic model\n",
    "dice = pm.Categorical.dist(theta_2)\n",
    "print(\">\",Omega[dice.random()])#result of one roll\n",
    "print(\">\",Omega[dice.random()])#result of another roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [15. 13. 16. 17. 26. 13.]\n",
      "> [ 8. 18. 17. 22. 24. 11.]\n"
     ]
    }
   ],
   "source": [
    "#we select probabilities for the six faces\n",
    "theta = np.array([0.1,0.2,0.2,0.2,0.2,0.1])\n",
    "#we define a probabilistic model\n",
    "dice = pm.Multinomial.dist(100,theta)\n",
    "print(\">\",dice.random())#result of 100 rolls\n",
    "print(\">\",dice.random())#result of other 100 rolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7223664828696717e-79"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(theta**([15., 13., 16., 17., 26., 13.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-180.3519122990885"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.log(theta**([15., 13., 16., 17., 26., 13.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13. 17. 19. 27. 17.  7.]\n",
      "MLE= [0.13 0.17 0.19 0.27 0.17 0.07]\n",
      "[10105. 19843. 20176. 19844. 19933. 10099.]\n",
      "MLE= [0.10105 0.19843 0.20176 0.19844 0.19933 0.10099]\n"
     ]
    }
   ],
   "source": [
    "#we select probabilities for the six faces\n",
    "theta = np.array([0.1,0.2,0.2,0.2,0.2,0.1])\n",
    "#we define a probabilistic model\n",
    "dice = pm.Multinomial.dist(100,theta)\n",
    "data=dice.random()\n",
    "print(data)#ounts for 100 rolls\n",
    "print(\"MLE=\",data/np.sum(data))\n",
    "#no we throws the dice 10000 times\n",
    "dice = pm.Multinomial.dist(100000,theta)\n",
    "data=dice.random()\n",
    "print(data)#counts for 100000 rolls\n",
    "print(\"MLE=\",data/np.sum(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -11.992, ||grad|| = 13.657: 100%|██████████| 9/9 [00:00<00:00, 2706.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11594203 0.23913043 0.12318841 0.13043478 0.19565217 0.19565217] [0.11363636 0.24242425 0.12121209 0.1287879  0.1969697  0.1969697 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "counts = np.array([15, 32, 16, 17, 26, 26])\n",
    "alpha = np.ones(len(counts),float)\n",
    "with pm.Model() as model:\n",
    "    theta=pm.Dirichlet(\"theta\",alpha)\n",
    "    data =pm.Multinomial(\"likelihood\",np.sum(counts),theta,observed=counts)\n",
    "#MAP estimate analytical formula\n",
    "MAP=(counts+alpha)/np.sum(counts+alpha)\n",
    "#we can also do it numerically\n",
    "map_estimate = pm.find_MAP(model=model)\n",
    "print(MAP,map_estimate['theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10. 15. 24. 30. 14.  7.]\n",
      "MAP= [0.10377358 0.1509434  0.23584906 0.29245283 0.14150943 0.0754717 ]\n",
      "[10031. 20208. 20007. 19844. 19934.  9976.]\n",
      "MAP= [0.10031398 0.20207788 0.200068   0.19843809 0.19933804 0.09976401]\n"
     ]
    }
   ],
   "source": [
    "#we select probabilities for the six faces\n",
    "theta = np.array([0.1,0.2,0.2,0.2,0.2,0.1])\n",
    "#we define a probabilistic model\n",
    "dice = pm.Multinomial.dist(100,theta)\n",
    "data=dice.random()\n",
    "print(data)#ounts for 100 rolls\n",
    "alpha = np.ones(len(data),float)\n",
    "print(\"MAP=\",(data+alpha)/np.sum(data+alpha))\n",
    "#no we throws the dice 10000 times\n",
    "dice = pm.Multinomial.dist(100000,theta)\n",
    "data=dice.random()\n",
    "print(data)#counts for 100000 rolls\n",
    "print(\"MAP=\",(data+alpha)/np.sum(data+alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "? pm.find_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam filter with one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.75]\n",
      " [0.5  0.5 ]]\n",
      "[1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benavoli/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "X=np.array([[1,1,1,0,0,1,0,0]]).T# 1 Won included, 0 not included\n",
    "y=np.array( [1,1,0,0,1,1,1,0])#1 spam, 0 not spam\n",
    "Xtest=np.array([[1,0]]).T\n",
    "clf = BernoulliNB(alpha=0.0,fit_prior=True)\n",
    "clf.fit(X, y)\n",
    "print(clf.predict_proba(Xtest))\n",
    "print(clf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5.]\n",
      "[[1.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.class_count_)\n",
    "print(clf.feature_count_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.75]\n",
      " [0.5  0.5 ]]\n",
      "[1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benavoli/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "X=np.array([[1,0],[1,0],[1,0],[0,1],[0,1],[1,0],[0,1],[0,1]])# 1 Won included, 0 not included\n",
    "y=np.array( [1,1,0,0,1,1,1,0])#1 spam, 0 not spam\n",
    "Xtest=np.array([[1,0],[0,1]]).T\n",
    "clf = MultinomialNB(alpha=0.0,fit_prior=True)\n",
    "clf.fit(X, y)\n",
    "print(clf.predict_proba(Xtest))\n",
    "print(clf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5.]\n",
      "[[1. 0.]\n",
      " [3. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.class_count_)\n",
    "print(clf.feature_count_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29577465 0.70422535]\n",
      " [0.45652174 0.54347826]]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "X=np.array([[1,0],[1,0],[1,0],[0,1],[0,1],[1,0],[0,1],[0,1]])# 1 Won included, 0 not included\n",
    "y=np.array( [1,1,0,0,1,1,1,0])#1 spam, 0 not spam\n",
    "Xtest=np.array([[1,0],[0,1]]).T\n",
    "clf = MultinomialNB(alpha=1.0,fit_prior=True)\n",
    "clf.fit(X, y)\n",
    "print(clf.predict_proba(Xtest))\n",
    "print(clf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.00000000e-11 1.00000000e+00]\n",
      " [1.00000000e+00 3.33333333e-11]]\n",
      "[1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benavoli/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "X=np.array([[1,0],[1,0],[0,1],[0,1],[1,0],[1,0],[1,0],[0,1]])# 1 Won included, 0 not included\n",
    "y=np.array( [1,1,0,0,1,1,1,0])#1 spam, 0 not spam\n",
    "Xtest=np.array([[1,0],[0,1]]).T\n",
    "clf = MultinomialNB(alpha=0.0,fit_prior=True)\n",
    "clf.fit(X, y)\n",
    "print(clf.predict_proba(Xtest))\n",
    "print(clf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of MultinomialNB from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29577465, 0.70422535],\n",
       "       [0.45652174, 0.54347826]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class MultinomialNB(object):\n",
    "    #class constructor\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha#set the smoothing parameter\n",
    "    #fit method\n",
    "    def fit(self, X, y):\n",
    "        count_sample = X.shape[0]\n",
    "        split_per_class = [[x for x, t in zip(X, y) if t == c]\n",
    "                    for c in np.unique(y)]\n",
    "        \n",
    "        self.class_log_prior_ = [np.log(len(i) / count_sample) for i in split_per_class]\n",
    "        count = np.array([np.array(i).sum(axis=0) for i in split_per_class]) + self.alpha\n",
    "        self.feature_log_prob_ = np.log(count / count.sum(axis=1)[np.newaxis].T)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        proba= [np.exp((self.feature_log_prob_ * x).sum(axis=1) + self.class_log_prior_)\n",
    "                for x in X]\n",
    "        prob=[p/np.sum(p) for p in proba]\n",
    "        return np.vstack(prob)     \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)\n",
    "    \n",
    "X=np.array([[1,0],[1,0],[1,0],[0,1],[0,1],[1,0],[0,1],[0,1]])\n",
    "y=np.array( [1,1,0,0,1,1,1,0])#1 spam, 0 not spam\n",
    "Xtest=np.array([[1,0],[0,1]]).T\n",
    "\n",
    "MNB = MultinomialNB(1) \n",
    "MNB.fit(X,y)\n",
    "MNB.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100' 'are' 'ashley' 'buy' 'bye' 'car' 'charlie' 'clothes' 'cruise'\n",
      " 'dear' 'discount' 'documents' 'euros' 'for' 'game' 'helen' 'hi' 'huge'\n",
      " 'john' 'lottery' 'new' 'offer' 'our' 'ready' 'thank' 'the' 'we' 'won'\n",
      " 'yesterday' 'you' 'your']\n",
      "[[1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      " [0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]]\n",
      "[1 1 1 2 3 1 1 1 1 3 1 1 1 2 1 1 1 1 1 1 2 1 2 1 1 2 1 4 1 5 2]\n"
     ]
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "D=[\"Dear John You won 100 euros\", \"Dear Ashley You won a new car\",\\\n",
    "   \"Hi Helen We won the game yesterday Bye\",\"Huge discount for you buy our new clothes Bye\",\n",
    " \"Thank you for your offer\", \"You won the lottery\",\n",
    " \"Buy our cruise\",  \"Dear Charlie Your documents are ready Bye\"]\n",
    "Corpus = [word.lower() for word in D]# we make everything lowercase\n",
    "vectorizer=CountVectorizer()\n",
    "R=vectorizer.fit_transform(Corpus).toarray()\n",
    "WordsList=np.array(vectorizer.get_feature_names())\n",
    "print(WordsList)\n",
    "print(R)#tells us which words are present in each email\n",
    "print(np.sum(R,axis=0))#total counts of each word in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0.]\n",
      "> ['dear' 'our' 'won' 'you' 'your']\n",
      "> ['euros' 'new' 'our' 'thank' 'you']\n",
      "> ['ashley' 'buy' 'cruise' 'game' 'new']\n"
     ]
    }
   ],
   "source": [
    "Counts = np.sum(R,axis=0)\n",
    "#\n",
    "theta = Counts/np.sum(Counts)#MLE estimate of theta\n",
    "#we define a probabilistic model\n",
    "sentence = pm.Multinomial.dist(5,theta)\n",
    "print(sentence.random())\n",
    "print(\">\",WordsList[sentence.random()==1])#generate some random emails with 5 words\n",
    "print(\">\",WordsList[sentence.random()==1])#generate some random emails with 5 words\n",
    "print(\">\",WordsList[sentence.random()==1])#generate some random emails with 5 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100 euros' 'are ready' 'ashley you' 'buy our' 'charlie your'\n",
      " 'clothes bye' 'dear ashley' 'dear charlie' 'dear john' 'discount for'\n",
      " 'documents are' 'for you' 'for your' 'game yesterday' 'helen we'\n",
      " 'hi helen' 'huge discount' 'john you' 'new car' 'new clothes'\n",
      " 'our cruise' 'our new' 'ready bye' 'thank you' 'the game' 'the lottery'\n",
      " 'we won' 'won 100' 'won new' 'won the' 'yesterday bye' 'you buy'\n",
      " 'you for' 'you won' 'your documents' 'your offer']\n"
     ]
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "D=[\"Dear John You won 100 euros\", \"Dear Ashley You won a new car\",\\\n",
    "   \"Hi Helen We won the game yesterday Bye\",\"Huge discount for you buy our new clothes Bye\",\n",
    " \"Thank you for your offer\", \"You won the lottery\",\n",
    " \"Buy our cruise\",  \"Dear Charlie Your documents are ready Bye\"]\n",
    "Corpus = [word.lower() for word in D]# we make everything lowercase\n",
    "vectorizer=CountVectorizer(ngram_range=(1,1))#unigrams\n",
    "R=vectorizer.fit_transform(Corpus).toarray()\n",
    "WordsList=np.array(vectorizer.get_feature_names())\n",
    "\n",
    "vectorizer=CountVectorizer(ngram_range=(2,2))#bigrams\n",
    "R=vectorizer.fit_transform(Corpus).toarray()\n",
    "Bigrams=np.array(vectorizer.get_feature_names())\n",
    "print(Bigrams)\n",
    "Frequencies=np.zeros((WordsList.shape[0],WordsList.shape[0]))\n",
    "for i in range(len(WordsList)):\n",
    "    for j in range(len(WordsList)):\n",
    "        if WordsList[i]+' '+WordsList[j] in Bigrams:\n",
    "            Frequencies[i,j]=Frequencies[i,j]+1\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dear', 'charlie', 'your', 'documents', 'are', 'ready']\n",
      "['hi', 'helen', 'we', 'won', '100', 'euros']\n"
     ]
    }
   ],
   "source": [
    "theta=(Frequencies+0.0001)/np.sum(Frequencies+0.0001,axis=1)[:,None]  #MLE with smoothing \n",
    "word=\"dear\"\n",
    "Sentence=[word]\n",
    "for i in range(5):\n",
    "    ind=np.where((word == WordsList)==True)[0]\n",
    "    word = WordsList[np.where(pm.Multinomial.dist(1,theta[ind,:]).random()==1)[1]][0]\n",
    "    Sentence.append(word)\n",
    "print(Sentence)\n",
    "word=\"hi\"\n",
    "Sentence=[word]\n",
    "for i in range(5):\n",
    "    ind=np.where((word == WordsList)==True)[0]\n",
    "    word = WordsList[np.where(pm.Multinomial.dist(1,theta[ind,:]).random()==1)[1]][0]\n",
    "    Sentence.append(word)\n",
    "print(Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lottery'], dtype='<U9')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordsList[np.where(pm.Multinomial.dist(1,theta[ind,:]).random()==1)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([7]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(pm.Multinomial.dist(1,theta[ind,:]).random()==1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.Multinomial.dist(1,theta[ind,:]).random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['red','brown','brown','white']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
